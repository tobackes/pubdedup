from collections import Counter
import os,sys
import re
import csv
import itertools
import operator
from copy import deepcopy as copy
import sqlite3
import numpy as np
from scipy.sparse import csr_matrix as csr
from scipy.sparse.csgraph import connected_components
import multiprocessing as MP
import time

inDB    = sys.argv[1];
#freqDB  = sys.argv[2];
phrfile = sys.argv[2];
trafile = sys.argv[3];
modfile = sys.argv[4];

CNT = 0; TRE = 1; PRO = 2;

_max_len_ = 4;
_n_       = 3;
_jobs     = 8;
_jobs2    = 64;
_batch    = 10000;
_batch2   = 10000;

WORD = re.compile(r'(\b[^\s]+\b)');

def ngrams(seq,n):
    return [tuple(seq[i-n:i]) for i in xrange(n,len(seq)+1) ];

def probs_leq(p,c,tree):
    return [(w,w_) for w in tree for w_ in tree[w][TRE] if tree[w][TRE][w_][PRO]>=p and tree[w][PRO]>=c];

def make_tree(d):
    tree = dict();
    for ngram in d:
        if len(ngram) == 2:
            if ngram[0] in tree:
                tree[ngram[0]][CNT] += d[ngram];
                if ngram[1] in tree[ngram[0]][TRE]:
                    tree[ngram[0]][TRE][ngram[1]][CNT] += d[ngram];
                else:
                    tree[ngram[0]][TRE][ngram[1]] = [d[ngram],dict(),0];
            else:
                tree[ngram[0]] = [d[ngram],{ngram[1]:[d[ngram],dict(),0]},0];
    divisor = float(sum([tree[w][CNT] for w in tree]));
    for w in tree:
        tree[w][PRO] = tree[w][CNT] / divisor;
        for w_ in tree[w][TRE]:
            tree[w][TRE][w_][PRO] = float(tree[w][TRE][w_][CNT]) / tree[w][CNT];
    return tree;

def display(p,c,tree,inversed=False):
    if inversed:
        for w,w_ in probs_leq(p,c,tree):
            print '(inv)', w_,w, tree[w][CNT], tree[w][TRE][w_][CNT], tree[w][TRE][w_][PRO];
    else:
        for w,w_ in probs_leq(p,c,tree):
            print '(std)', w,w_, tree[w][CNT], tree[w][TRE][w_][CNT], tree[w][TRE][w_][PRO];

def transitive_closure(M): # WARNING: Not for large M!
    labels  = connected_components(M)[1];
    closure = csr(labels==labels[:,None]);
    return closure;

def term_transitions(replace,DIST='damerau'):
    index2term = list(set([item for item in replace.keys()]) | set([item for item in replace.values()]));
    term2index = {index2term[i]:i for i in xrange(len(index2term))};
    rows, cols = zip(*[[term2index[item[0]],term2index[item[1]]] for item in replace.iteritems()]);
    R              = csr((np.ones(2*len(rows)),(rows+cols,cols+rows)),dtype=bool,shape=(len(index2term),len(index2term)));
    labels         = connected_components(R)[1];
    sorting        = np.argsort(labels);
    labels_s       = labels[sorting];
    _, starts      = np.unique(labels_s,return_index=True);
    sizes          = np.diff(starts);
    groups         = [group for group in np.split(sorting,starts[1:]) if group.size > 1];
    transition     = dict();
    for group in groups:
        sum_group = float(sum([d[(index2term[index],)] for index in group]));
        max_index = None;
        max_freq  = 0;
        for index in group:
            predict_term = index2term[index];
            predict_freq = d[(predict_term,)];
            if predict_freq > max_freq:
                max_freq  = predict_freq;
                max_index = index;
        for index1 in group:
            given_term             = index2term[index1];
            len_1                  = len(given_term);
            transition[given_term] = dict();
            for index2 in [index1,max_index]:
                predict_term                         = index2term[index2];
                len_2                                = len(predict_term);
                sim_prefix                           = prefix_normed(     given_term,predict_term,len_1,len_2      );
                sim_similar                          = similarity_normed( given_term,predict_term,len_1,len_2,DIST );
                transition[given_term][predict_term] = (d[(predict_term,)]/sum_group) * sim_similar;#(sim_similar+sim_prefix)/2;
            sum_sim = sum([transition[given_term][predict_term] for predict_term in transition[given_term]]);
            for predict_term in transition[given_term]:
                transition[given_term][predict_term] /= sum_sim;
            for index2 in [index1,max_index]:
                print given_term, '-->', index2term[index2], transition[given_term][index2term[index2]];
    return transition;

def apply_replace(index2term,replacements):
    if len(replacements) == 0:
        return dict();
    term2index     = {index2term[i]:i for i in xrange(len(index2term))};
    rows,cols,sims = zip(*replacements);
    R              = csr((np.ones(2*len(rows)),(rows+cols,cols+rows)),dtype=bool,shape=(len(index2term),len(index2term)));
    labels         = connected_components(R)[1];
    sorting        = np.argsort(labels);
    labels_s       = labels[sorting];
    _, starts      = np.unique(labels_s,return_index=True);
    sizes          = np.diff(starts);
    groups         = [group for group in np.split(sorting,starts[1:]) if group.size > 1];
    replace        = dict();
    for group in groups:
        terms = [index2term[i] for i in group];
        repre = max([(d[(term,)],term) for term in terms])[1];
        for term in terms:
            if term != repre:
                replace[term] = repre;
    return replace;

def replace_by_prefix(index2term,threshold,window):
    replacements = set([]);
    for i in xrange(len(index2term)-window):
        len_1 =  len(index2term[i]);
        for j in xrange(1,window+1):
            len_2   =  len(index2term[i+j]);
            percent = prefix_normed(index2term[i],index2term[i+j],len_1,len_2);
            if percent > threshold:
                replacements.add((i+j,i,percent,));
    return replacements;

def replace_by_similar_(index2term,threshold,window,DIST):
    replacements = set([]);
    for i in xrange(len(index2term)-window):
        len_1     =  len(index2term[i]);
        for j in xrange(1,window+1):
            len_2   =  len(index2term[i+j]);
            percent = similarity_normed(index2term[i],index2term[i+j],len_1,len_2,DIST);
            if percent > threshold:
                replacements.add((i+j,i,percent,));
    return replacements;

def replace_by_similar(index2term,threshold,window,DIST,compared):
    replacements = set([]);
    tasks        = MP.Queue();
    results      = MP.Queue();
    T            = [];
    for i in xrange(len(index2term)-window):
        T += [(i+j,i,index2term[i],index2term[i+j],len(index2term[i]),len(index2term[i+j]),threshold,DIST,) for j in xrange(1,window+1) if not (index2term[i+j],index2term[i],) in compared];
        if len(T) > _batch2:
            compared |= set([(index2term[ij],index2term[i],) for ij,i,term_i,term_ij,len_1,len_2,threshold,DIST in T]);
            tasks.put(T);
            T = [];
    if len(T) != 0:
        tasks.put(T);
    workers = [MP.Process(target=get_similarity_normed,args=(tasks,results,x,)) for x in xrange(_jobs2)];
    for worker in workers:
        worker.start();
    for x in xrange(_jobs2):
        result = results.get();
        replacements |= result;
        #print 'Got result', x;
    for x in xrange(len(workers)):
        workers[x].join();
        #print 'Joined worker', x;
    return replacements, compared;

def get_similarity_normed(tasks,results,x):
    replacements = set([]);
    while True:
        print x,'says: Approximate number of jobs in queue:', tasks.qsize();
        try:
            T = tasks.get(timeout=3);
            #print x,'says: Got', len(T), 'tasks to do...';
        except:
            break;
        for ij,i,term_i,term_ij,len_1,len_2,threshold,DIST in T:
            percent = similarity_normed(term_i,term_ij,len_1,len_2,DIST);
            if percent > threshold:
                replacements.add((ij,i,percent,));
        #print x,'says: Done with this set of tasks.';
    #print 'Closing job', x;
    results.put(replacements);
    return 0;

def prefix_normed(term1,term2,len_1,len_2):
    prefix  =  os.path.commonprefix([term1,term2]);
    is_prefix = len(prefix)==min([len_1,len_2]);
    return float(len(prefix))/max([len_1,len_2]) if is_prefix else 0.0;

def similarity_normed(term1,term2,len_1,len_2,DIST):
    distance = damerau_dist(term1,term2) if DIST=='damerau' else edit_dist(term1,term2);
    return 1.-(float(distance)/max([len_1,len_2]));

def edit_dist(s1,s2):
    if len(s1) > len(s2):
        s1, s2 = s2, s1;
    distances = range(len(s1) + 1);
    for i2, c2 in enumerate(s2):
        distances_ = [i2+1];
        for i1, c1 in enumerate(s1):
            if c1 == c2:
                distances_.append(distances[i1]);
            else:
                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])));
        distances = distances_;
    return distances[-1];

def damerau_dist(s1,s2):
    oneago  = None;
    thisrow = range(1,len(s2)+1)+[0];
    for x in xrange(len(s1)):
        twoago, oneago, thisrow = oneago, thisrow, [0]*len(s2)+[x + 1];
        for y in xrange(len(s2)):
            delcost    = oneago[y] + 1;
            addcost    = thisrow[y-1] + 1;
            subcost    = oneago[y-1] + (s1[x]!=s2[y]);
            thisrow[y] = min(delcost,addcost,subcost);
            if (x>0 and y>0 and s1[x]==s2[y-1] and s1[x-1]==s2[y] and s1[x]!=s2[y]):
                thisrow[y] = min(thisrow[y],twoago[y-2]+1);
    return thisrow[len(s2)-1];

def replace_in_d(d,replace,threshold):
    denom = float(sum([d[key] for key in d]));
    d_    = Counter();
    for tup in d:
        d_[tuple([replace[term] if term in replace and d[term]/denom < threshold else term for term in tup])] += d[tup];
    return d_;

def add_in_d(d,transition):
    new_d    = dict();
    for tup in d:
        options = [els for els in itertools.product(*[[(term2,transition[term1][term2],) for term2 in transition[term1]] if term1 in transition else [(term1,1.,)] for term1 in tup])]
        for option in options:
            tup_, weights = zip(*option);
            new_d[tup_]   = reduce(operator.mul,weights,1)*d[tup];
    d_ = {tup:new_d[tup] if tup in new_d else d[tup] for tup in set(d.keys())|set(new_d.keys())};
    return d_;

def build_counts(Q,R,x,):
    print 'started', x;
    d, d_inv = Counter(), Counter();
    while True:
        print x,'says: Approximate number of jobs in queue:', Q.qsize();
        try:
            rows = Q.get(timeout=3);
        except:
            break;
        titles = [[term.lower() for term in re.findall(WORD,row[0])] for row in rows if not row[0]==None];
        for title in titles:
            if len(title)==0: continue;
            for n in xrange(1,_n_):
                for ngram in ngrams(title,n):
                    d[ngram]                      += 1;
                    d_inv[tuple(reversed(ngram))] += 1;
        if len(d) > 100000:
            R.put((d,d_inv,));
            d, d_inv = Counter(), Counter();
    print 'Closing job', x;
    R.put((d,d_inv,));
    return 0;

def feed(Q,inDB):
    con_title = sqlite3.connect(inDB);
    cur_title = con_title.cursor();
    cur_title.execute("SELECT title FROM publications WHERE title IS NOT NULL");
    i = 0;
    while True:
        i += 1; print i*_batch;
        rows = cur_title.fetchmany(_batch);
        if len(rows) == 0:
            break;
        Q.put(rows);#Q.put(row[0]);
        while Q.qsize() > 1000:
            time.sleep(1);
    con_title.close();
    return 0;


Q = MP.Queue();
R = MP.Queue();

feeder  = MP.Process(target=feed,args=(Q,inDB,));
workers = [MP.Process(target=build_counts,args=(Q,R,x,)) for x in xrange(_jobs)];

feeder.start(); time.sleep(5);

for worker in workers:
    worker.start();

feeder.join();

counts = [];
for x in xrange(_jobs):
    result = R.get();
    counts.append(result);

for x in xrange(len(workers)):
    workers[x].join();
    print 'Joined worker', x;

print 'Combining results';print 1;
d     = sum((counts[x][0] for x in xrange(len(counts))),Counter());print 2;
d_inv = sum((counts[x][1] for x in xrange(len(counts))),Counter());
print 'Done combining results.';
raw_input('Press Enter to continue...');

#-------------------------------------------------------------------------------------

_threshold_prefix  = 0.75#0.8; #TODO: These thresholds might be different for each typ
_window_prefix     = 1;
_threshold_similar = 0.8#.875;  #TODO: Try out what are the best ones for each type!
_window_similar    = 20;
_distance          = 'damerau'; #'edit'

terms         = sorted([gram[0] for gram in d if len(gram)==1]);
index2term    = copy(terms);
term2index    = {terms[i]:i for i in xrange(len(terms))};
term2index_   = copy(term2index);
replace       = dict();
sim_prefix    = dict();
sim_similar   = dict();
num_replace   = 99;
compared      = set();
while num_replace > 0:
    print 1;
    replace_prefix  = replace_by_prefix( terms,_threshold_prefix,_window_prefix);
    print 2;
    replace_similar, compared = replace_by_similar(terms,_threshold_similar,_window_similar,_distance,compared);
    #replace_edit    = replace_by_similar(terms,_threshold_similar,_window_similar,'edit');
    print 3;
    replace_new     = apply_replace(terms,replace_prefix|replace_similar);
    print 4;
    num_replace     = len(replace_new);
    print 5;
    only_prefix     = [(terms[pair[0]],terms[pair[1]],) for pair in replace_prefix-replace_similar];
    #only_damerau    = [(terms[pair[0]],terms[pair[1]],) for pair in replace_similar-replace_edit];
    print 6;
    replace.update(replace_new);
    print 7;
    terms       = sorted(list(set([replace[term] if term in replace else term for term in terms])));
    print 8;
    term2index_ = {terms[i]:i for i in xrange(len(terms))};
    print num_replace, '(',len(only_prefix),len(compared),')'#, '(',len(only_damerau),')';

transition = term_transitions(replace);

_replace_thr = 1.;#0.00001; The frequency of the replaced item is in no way indicative of wether the replace makes sense or not.

d     = add_in_d(d    ,transition);#replace_in_d(d,    replace,_replace_thr);
d_inv = add_in_d(d_inv,transition);#replace_in_d(d_inv,replace,_replace_thr);

#-------------------------------------------------------------------------------------

tree     = make_tree(d);
tree_inv = make_tree(d_inv);

display(1,10,tree);
display(1,10,tree_inv,True);

#-------------------------------------------------------------------------------------
_and_p_ = 0.5#0.6;
_and_c_ = 0.0001;
_or_p_  = 0.3#0.4;
_or_c_  = 0.0001;
#-------------------------------------------------------------------------------------
#-right-min-left-min------------------------------------------------------------------
set_std   = set(probs_leq(_and_p_,_and_c_,tree));
set_inv   = set([tuple(reversed(el)) for el in probs_leq(_and_p_,_and_c_,tree_inv)]);
inter     = set_std & set_inv;
#-------------------------------------------------------------------------------------
#-right-certain-left-min--------------------------------------------------------------
set_std_  = set(probs_leq(1.0,_or_c_,tree));
set_inv_  = set([tuple(reversed(el)) for el in probs_leq(_or_p_,_or_c_,tree_inv)]);
inter_    = set_std_ & set_inv_;
#-left-certain-right-min--------------------------------------------------------------
#-------------------------------------------------------------------------------------
set_std__ = set(probs_leq(_or_p_,_or_c_,tree));
set_inv__ = set([tuple(reversed(el)) for el in probs_leq(1.0,_or_c_,tree_inv)]);
inter__   = set_std__ & set_inv__;
#-------------------------------------------------------------------------------------
union     = inter | inter_ | inter__;
#-------------------------------------------------------------------------------------
print union;
print len(inter), '+', len(inter_), '->', len(inter|inter_), '+', len(inter__), '->', len(union);

OUT = open(phrfile,'w');
for tup in union:
    OUT.write(tup[0]+' '+tup[1]+'\n');
OUT.close();

OUT = open(trafile,'w');
for key in replace:
    OUT.write(key.encode('utf-8')+' '+replace[key].encode('utf-8')+'\n');
OUT.close();

OUT = open(modfile,'w');
for term1 in transition:
    for term2 in transition[term1]:
        OUT.write(term1.encode('utf-8')+' '+term2.encode('utf-8')+' '+str(transition[term1][term2])+'\n');
OUT.close();
'''

lines_ = [];
for line in lines:
    if len(line)<=1:
        lines_.append(line);
        continue;
    else:
        line_ = [];
    bigrams = ngrams(line,2);
    phrase  = bigrams[0][0];
    for bigram in bigrams:
        if bigram in union:
            phrase += '_'+bigram[1];
        else:
            line_ += [phrase];
            phrase = bigram[1];
    if len(line_)==0 or not line_[-1].endswith(phrase): line_.append(phrase);
    lines_.append(line_);

reps = [];
for i in xrange(len(lines_)):
    rep = set([string for string in lines_[i] if len(string)>=3]) - illeg;
    reps.append(rep);

for i in xrange(len(reps)):
    if len(reps[i])>_max_len_:
        rep = set([tup[1] for tup in sorted([(d[el],el) for el in rep])][:_max_len_]);
'''
